{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = r'../data/yahoo_top_products_click_popularity_20200201.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\echon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(input_file, sep=r'\\t')\n",
    "\n",
    "# temp descriptions\n",
    "#df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parent L1 Description</th>\n",
       "      <th>Parent L2 Description</th>\n",
       "      <th>MID</th>\n",
       "      <th>Merchant Name</th>\n",
       "      <th>OID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Manufacturer</th>\n",
       "      <th>Price</th>\n",
       "      <th>Min Offer Price 30 Day</th>\n",
       "      <th>Price Change</th>\n",
       "      <th>Click Popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Pet Supplies</td>\n",
       "      <td>More Pet Supplies</td>\n",
       "      <td>248942</td>\n",
       "      <td>Chewy.com</td>\n",
       "      <td>9151715234</td>\n",
       "      <td>Wisdom Panel Health Breed &amp; Health Identificat...</td>\n",
       "      <td>Wisdom Panel Health Breed &amp; Health Identificat...</td>\n",
       "      <td>Wisdom Panel</td>\n",
       "      <td>149.99</td>\n",
       "      <td>89.99</td>\n",
       "      <td>66.67</td>\n",
       "      <td>35415.0480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Clothing &amp; Accessories</td>\n",
       "      <td>Men's Clothing</td>\n",
       "      <td>313027</td>\n",
       "      <td>eBay PLA US</td>\n",
       "      <td>11114337071</td>\n",
       "      <td>Girl Scout Cookies 2019-20 New Cookies are in!...</td>\n",
       "      <td>Girl Scout Cookies 2019-20 New Cookies are in!...</td>\n",
       "      <td>Girl Scouts</td>\n",
       "      <td>48.00</td>\n",
       "      <td>48.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12215.2644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Clothing &amp; Accessories</td>\n",
       "      <td>Men's Clothing</td>\n",
       "      <td>134716</td>\n",
       "      <td>MensUSA.com</td>\n",
       "      <td>5651235052</td>\n",
       "      <td>Classic Long Royal Blue Fashion Zoot Suit</td>\n",
       "      <td>\"This Zoot Suit is as nice and unique as it ge...</td>\n",
       "      <td>mensusa</td>\n",
       "      <td>139.00</td>\n",
       "      <td>139.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8317.7750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>'N/A'</td>\n",
       "      <td>'N/A'</td>\n",
       "      <td>76071</td>\n",
       "      <td>Joe's New Balance Outlet</td>\n",
       "      <td>9756405838</td>\n",
       "      <td>New Balance Women's FuelCore NERGIZE Shoes Bla...</td>\n",
       "      <td>Slip on the FuelCore NERGIZE women's training ...</td>\n",
       "      <td>New Balance</td>\n",
       "      <td>38.99</td>\n",
       "      <td>38.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6132.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Clothing &amp; Accessories</td>\n",
       "      <td>Handbags &amp; Luggage</td>\n",
       "      <td>31851</td>\n",
       "      <td>Kohl's</td>\n",
       "      <td>5681651813</td>\n",
       "      <td>Stone &amp; Co. Irene Leather Hobo, Grey</td>\n",
       "      <td>Watch the product video here. Stone &amp; Co. embo...</td>\n",
       "      <td>Stone &amp; Co.</td>\n",
       "      <td>99.00</td>\n",
       "      <td>69.30</td>\n",
       "      <td>42.85</td>\n",
       "      <td>5251.5413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Parent L1 Description Parent L2 Description     MID  \\\n",
       "0            Pet Supplies     More Pet Supplies  248942   \n",
       "1  Clothing & Accessories        Men's Clothing  313027   \n",
       "2  Clothing & Accessories        Men's Clothing  134716   \n",
       "3                   'N/A'                 'N/A'   76071   \n",
       "4  Clothing & Accessories    Handbags & Luggage   31851   \n",
       "\n",
       "              Merchant Name          OID  \\\n",
       "0                 Chewy.com   9151715234   \n",
       "1               eBay PLA US  11114337071   \n",
       "2               MensUSA.com   5651235052   \n",
       "3  Joe's New Balance Outlet   9756405838   \n",
       "4                    Kohl's   5681651813   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Wisdom Panel Health Breed & Health Identificat...   \n",
       "1  Girl Scout Cookies 2019-20 New Cookies are in!...   \n",
       "2          Classic Long Royal Blue Fashion Zoot Suit   \n",
       "3  New Balance Women's FuelCore NERGIZE Shoes Bla...   \n",
       "4               Stone & Co. Irene Leather Hobo, Grey   \n",
       "\n",
       "                                         Description  Manufacturer   Price  \\\n",
       "0  Wisdom Panel Health Breed & Health Identificat...  Wisdom Panel  149.99   \n",
       "1  Girl Scout Cookies 2019-20 New Cookies are in!...   Girl Scouts   48.00   \n",
       "2  \"This Zoot Suit is as nice and unique as it ge...       mensusa  139.00   \n",
       "3  Slip on the FuelCore NERGIZE women's training ...   New Balance   38.99   \n",
       "4  Watch the product video here. Stone & Co. embo...   Stone & Co.   99.00   \n",
       "\n",
       "   Min Offer Price 30 Day  Price Change  Click Popularity  \n",
       "0                   89.99         66.67        35415.0480  \n",
       "1                   48.00          0.00        12215.2644  \n",
       "2                  139.00          0.00         8317.7750  \n",
       "3                   38.99          0.00         6132.8540  \n",
       "4                   69.30         42.85         5251.5413  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Click Popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Seiko Blue Men's Silver-Tone Blue Dial Chronog...</td>\n",
       "      <td>119.99</td>\n",
       "      <td>0.1932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1999-2005 Volkswagen Jetta Clutch Kit - LUK 17...</td>\n",
       "      <td>201.43</td>\n",
       "      <td>0.3865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Madden Girl Bounce Platform Sneakers - Tan</td>\n",
       "      <td>34.50</td>\n",
       "      <td>0.1932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>G.H. Lammerse Dahlia - 2 per package</td>\n",
       "      <td>19.99</td>\n",
       "      <td>0.1932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Popcornopolis 6-Cone Holiday Popcorn Variety Pack</td>\n",
       "      <td>23.08</td>\n",
       "      <td>0.1932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title   Price  Click Popularity\n",
       "0  Seiko Blue Men's Silver-Tone Blue Dial Chronog...  119.99            0.1932\n",
       "1  1999-2005 Volkswagen Jetta Clutch Kit - LUK 17...  201.43            0.3865\n",
       "2         Madden Girl Bounce Platform Sneakers - Tan   34.50            0.1932\n",
       "3               G.H. Lammerse Dahlia - 2 per package   19.99            0.1932\n",
       "4  Popcornopolis 6-Cone Holiday Popcorn Variety Pack   23.08            0.1932"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Null Preprocessing\n",
    "null_rows = (df_train['Title'].isnull()) \\\n",
    "            | (df_train['Parent L1 Description'].isnull()) \\\n",
    "            | (df_train['Parent L2 Description'].isnull()) \\\n",
    "            | (df_train['Price'].isnull()) \\\n",
    "            | (df_train['Click Popularity'].isnull())\n",
    "df_train = df_train[~null_rows]\n",
    "\n",
    "# MAX LENGTH substitution\n",
    "#np.max(df_train['Title'].apply(lambda x : len(x)).to_list())\n",
    "MAX_LENGTH = 50\n",
    "df_train = df_train[~(df_train.Title.apply(lambda x : len(x)) > MAX_LENGTH)]\n",
    "\n",
    "# Sampling\n",
    "SAMPLE_FRAC = 0.1\n",
    "df_train = df_train.sample(frac=SAMPLE_FRAC, random_state=9487)\n",
    "\n",
    "# Column Selections\n",
    "df_train = df_train.reset_index()\n",
    "df_train = df_train.loc[:, ['Title', 'Price', 'Click Popularity']]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample number： 38481\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Click Popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Seiko Blue Men's Silver-Tone Blue Dial Chronog...</td>\n",
       "      <td>119.99</td>\n",
       "      <td>0.1932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1999-2005 Volkswagen Jetta Clutch Kit - LUK 17...</td>\n",
       "      <td>201.43</td>\n",
       "      <td>0.3865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Madden Girl Bounce Platform Sneakers - Tan</td>\n",
       "      <td>34.50</td>\n",
       "      <td>0.1932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>G.H. Lammerse Dahlia - 2 per package</td>\n",
       "      <td>19.99</td>\n",
       "      <td>0.1932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Popcornopolis 6-Cone Holiday Popcorn Variety Pack</td>\n",
       "      <td>23.08</td>\n",
       "      <td>0.1932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title   Price  Click Popularity\n",
       "0  Seiko Blue Men's Silver-Tone Blue Dial Chronog...  119.99            0.1932\n",
       "1  1999-2005 Volkswagen Jetta Clutch Kit - LUK 17...  201.43            0.3865\n",
       "2         Madden Girl Bounce Platform Sneakers - Tan   34.50            0.1932\n",
       "3               G.H. Lammerse Dahlia - 2 per package   19.99            0.1932\n",
       "4  Popcornopolis 6-Cone Holiday Popcorn Variety Pack   23.08            0.1932"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idempotence\n",
    "df_train.to_csv(\"../data/train.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"sample number：\", len(df_train))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset Generation\n",
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT-format + Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetTextDataset(Dataset):\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"test\"]\n",
    "        self.mode = mode\n",
    "        self.df = pd.read_csv(\"../data/\" + mode + \".tsv\", sep=\"\\t\").fillna(\"\")\n",
    "        self.len = len(self.df)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            text, price = self.df.iloc[idx, :2].values\n",
    "            reg_score_tensor = None\n",
    "        else:\n",
    "            text, price, reg_score = self.df.iloc[idx, :].values\n",
    "            reg_score_tensor = torch.tensor(reg_score)\n",
    "            \n",
    "        # BERT tokens + [SEP]\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        word_pieces += tokens + [\"[SEP]\"]\n",
    "        len_text = len(word_pieces)\n",
    "        \n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        segments_tensor = torch.tensor([0] * len_text, dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, reg_score_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TargetTextDataset(\"train\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_diff_dataset(dataset, index=0):\n",
    "    # 選擇第一個樣本\n",
    "    sample_idx = index\n",
    "\n",
    "    # 將原始文本拿出做比較\n",
    "    text_a, price_a, reg_score_a = trainset.df.iloc[sample_idx].values\n",
    "\n",
    "    # 利用剛剛建立的 Dataset 取出轉換後的 id tensors\n",
    "    tokens_tensor, segments_tensor, reg_score_tensor = trainset[sample_idx]\n",
    "\n",
    "    # 將 tokens_tensor 還原成文本\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "    combined_text = \" \".join(tokens)\n",
    "\n",
    "    # 渲染前後差異，毫無反應就是個 print。可以直接看輸出結果\n",
    "    print(f\"\"\"[原始文本]\n",
    "    句子 1：{text_a}\n",
    "    句子 2：{price_a}\n",
    "    分類  ：{reg_score_a}\n",
    "\n",
    "    --------------------\n",
    "\n",
    "    [Dataset 回傳的 tensors]\n",
    "    tokens_tensor  ：{tokens_tensor}\n",
    "\n",
    "    segments_tensor：{segments_tensor}\n",
    "\n",
    "    label_tensor   ：{reg_score_tensor}\n",
    "\n",
    "    --------------------\n",
    "\n",
    "    [還原 tokens_tensors]\n",
    "    {combined_text}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[原始文本]\n",
      "    句子 1：Seiko Blue Men's Silver-Tone Blue Dial Chronograph\n",
      "    句子 2：119.99\n",
      "    分類  ：0.1932\n",
      "\n",
      "    --------------------\n",
      "\n",
      "    [Dataset 回傳的 tensors]\n",
      "    tokens_tensor  ：tensor([  101,  7367, 12676,  2630,  2273,  1005,  1055,  3165,  1011,  4309,\n",
      "         2630, 13764, 10381,  4948,  8649, 24342,   102])\n",
      "\n",
      "    segments_tensor：tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "    label_tensor   ：0.1932000070810318\n",
      "\n",
      "    --------------------\n",
      "\n",
      "    [還原 tokens_tensors]\n",
      "    [CLS] se ##iko blue men ' s silver - tone blue dial ch ##ron ##og ##raph [SEP]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "check_diff_dataset(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 測試集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        reg_score = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        reg_score = None\n",
    "    \n",
    "    # zero pad 到同一序列長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, \n",
    "                                    batch_first=True)\n",
    "    \n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
    "                                dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(\n",
    "        tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, reg_score\n",
    "\n",
    "\n",
    "# 初始化一個每次回傳 64 個訓練樣本的 DataLoader\n",
    "# 利用 `collate_fn` 將 list of samples 合併成一個 mini-batch 是關鍵\n",
    "BATCH_SIZE = 64\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, \n",
    "                         collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dataloader(dataloader):\n",
    "    data = next(iter(dataloader))\n",
    "\n",
    "    tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, reg_score = data\n",
    "\n",
    "    print(f\"\"\"\n",
    "    tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "    {tokens_tensors}\n",
    "    ------------------------\n",
    "    segments_tensors.shape = {segments_tensors.shape}\n",
    "    {segments_tensors}\n",
    "    ------------------------\n",
    "    masks_tensors.shape    = {masks_tensors.shape}\n",
    "    {masks_tensors}\n",
    "    ------------------------\n",
    "    label_ids.shape        = {reg_score.shape}\n",
    "    {reg_score}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    tokens_tensors.shape   = torch.Size([64, 28]) \n",
      "    tensor([[  101,  7367, 12676,  ...,     0,     0,     0],\n",
      "        [  101,  2639,  1011,  ...,     0,     0,     0],\n",
      "        [  101, 24890,  2611,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  1056, 29602,  ...,     0,     0,     0],\n",
      "        [  101,  1996,  1017,  ...,     0,     0,     0],\n",
      "        [  101, 29450,  1058,  ...,     0,     0,     0]])\n",
      "    ------------------------\n",
      "    segments_tensors.shape = torch.Size([64, 28])\n",
      "    tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "    ------------------------\n",
      "    masks_tensors.shape    = torch.Size([64, 28])\n",
      "    tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "    ------------------------\n",
      "    label_ids.shape        = torch.Size([64])\n",
      "    tensor([ 0.1932,  0.3865,  0.1932,  0.1932,  0.1932,  0.1932,  0.1932,  0.1932,\n",
      "         2.3192,  0.1932,  0.1932,  0.1932,  0.3865,  0.1932,  0.1932,  0.9663,\n",
      "         0.7730,  0.7730,  0.1932,  1.1596,  0.1932,  0.3865,  0.3865,  0.1932,\n",
      "         0.1932,  0.1932,  1.1596,  0.3865,  0.1932,  0.7730,  0.1932,  0.7730,\n",
      "         8.6971,  0.1932,  0.3865,  0.5798,  0.1932,  0.1932,  0.1932,  0.1932,\n",
      "         0.9663,  0.1932,  0.1932,  0.1932,  0.1932,  0.3865,  0.1932,  0.9663,\n",
      "         5.2182,  0.3865, 13.5289,  0.1932,  0.1932,  0.1932,  0.1932,  2.3192,\n",
      "         2.7057,  0.9663,  0.1932,  0.3865,  2.7057,  0.1932,  0.1932,  0.1932])\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "check_dataloader(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "- Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name            module\n",
      "----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "dropout         Dropout(p=0.1, inplace=False)\n",
      "classifier      Linear(in_features=768, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# 載入一個可以做中文多分類任務的模型，n_class = 3\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "#PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
    "NUM_LABELS = 1\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "#clear_output()\n",
    "\n",
    "# high-level 顯示此模型裡的 modules\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"do_sample\": false,\n",
       "  \"eos_token_ids\": 0,\n",
       "  \"finetuning_task\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"is_decoder\": false,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"length_penalty\": 1.0,\n",
       "  \"max_length\": 20,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_beams\": 1,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_labels\": 1,\n",
       "  \"num_return_sequences\": 1,\n",
       "  \"output_attentions\": false,\n",
       "  \"output_hidden_states\": false,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pruned_heads\": {},\n",
       "  \"repetition_penalty\": 1.0,\n",
       "  \"temperature\": 1.0,\n",
       "  \"top_k\": 50,\n",
       "  \"top_p\": 1.0,\n",
       "  \"torchscript\": false,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_bfloat16\": false,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total_error = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            \n",
    "            # 別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "            # 且強烈建議在將這些 tensors 丟入 `model` 時指定對應的參數名稱\n",
    "            tokens_tensors, segments_tensors, masks_tensors, reg_score_tensors = data\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors,\n",
    "                            labels=reg_score_tensors)\n",
    "            \n",
    "            loss, logits = outputs[:2]\n",
    "            #_, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                total += reg_score_tensors.size(0)\n",
    "                total_error += float(loss.data) * total\n",
    "                \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = logits.data\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, logits.data))\n",
    "            \n",
    "            del tokens_tensors\n",
    "            del segments_tensors\n",
    "            del masks_tensors\n",
    "            del reg_score_tensors\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    if compute_acc:\n",
    "        omse = total_error / total\n",
    "        return predictions, omse\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讓模型跑在 GPU 上並取得訓練集的分類準確率\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "print(\"classification acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "整個分類模型的參數量：109483009\n",
      "線性分類器的參數量：769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_learnable_params(module):\n",
    "    return [p for p in module.parameters() if p.requires_grad]\n",
    "     \n",
    "model_params = get_learnable_params(model)\n",
    "clf_params = get_learnable_params(model.classifier)\n",
    "\n",
    "print(f\"\"\"\n",
    "整個分類模型的參數量：{sum(p.numel() for p in model_params)}\n",
    "線性分類器的參數量：{sum(p.numel() for p in clf_params)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] loss: 89431.751\n",
      "[epoch 2] loss: 89178.224\n",
      "[epoch 3] loss: 88589.665\n",
      "[epoch 4] loss: 87287.502\n",
      "[epoch 5] loss: 86109.136\n",
      "[epoch 6] loss: 84800.436\n",
      "Wall time: 28min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 訓練模式\n",
    "model.train()\n",
    "\n",
    "# 使用 Adam Optim 更新整個分類模型的參數\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "EPOCHS = 6  # 幸運數字\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        \n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        del outputs\n",
    "        del tokens_tensors\n",
    "        del segments_tensors\n",
    "        del masks_tensors\n",
    "        del labels\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # 計算分類準確率\n",
    "    #_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "\n",
    "    print('[epoch %d] loss: %.3f' %\n",
    "          (epoch + 1, running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\echon\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertForSequenceClassification. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\echon\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\echon\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\echon\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\echon\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\echon\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\echon\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\echon\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\echon\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\echon\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\echon\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertSelfAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\echon\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\echon\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertSelfOutput. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\echon\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertIntermediate. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\echon\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertOutput. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\echon\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertPooler. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\echon\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Tanh. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "torch.save(model.to(\"cpu\"), \"../model/bert_test_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 用分類模型預測測試集\n",
    "predictions = get_predictions(model, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictions.numpy()[:64, ].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_value = df_train['Click Popularity'][:64].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "sort_index = np.argsort(-1*true_value)\n",
    "y = np.array([2] * 30 + [1] * 34)\n",
    "scores = result[sort_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50, 32, 48, 60, 56,  8, 55, 19, 26, 47, 57, 40, 15, 29, 17, 16, 31,\n",
       "       35, 27, 12, 49, 21, 22, 34, 59,  1, 45, 41, 42, 43, 52, 61, 51, 46,\n",
       "       58, 54, 53, 44, 39,  0, 37,  2,  3,  4,  5,  6,  7,  9, 10, 11, 13,\n",
       "       14, 18, 20, 23, 24, 25, 28, 30, 62, 33, 36, 38, 63], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5786721 ,  0.4731172 ,  1.2724991 ,  0.5953915 ,  0.98217916,\n",
       "        2.859677  ,  0.77891695,  0.912122  ,  1.4449575 ,  0.6277391 ,\n",
       "        0.47311452,  0.56747794,  0.07988021,  0.11860785,  0.64048314,\n",
       "        0.45623112,  0.6713881 ,  0.6968294 ,  0.36715102,  0.6362337 ,\n",
       "        0.34025955,  0.33606663,  0.7585087 ,  0.5409566 ,  0.58195615,\n",
       "       12.652956  ,  0.48444462,  0.3234909 ,  1.0304863 , -0.28407213,\n",
       "        0.73470926,  0.5768399 , 12.88768   ,  0.48148265,  0.4336914 ,\n",
       "        0.22489761,  0.36202967,  0.78652614,  0.7793033 ,  0.30464888,\n",
       "        0.7858947 ,  0.50743526,  0.30857506,  0.24206609,  0.56826574,\n",
       "        0.4461549 ,  0.40718803,  1.3025625 ,  2.7594044 ,  0.43979415,\n",
       "        9.14373   ,  0.8396159 ,  0.4253715 ,  0.92645556,  2.0673487 ,\n",
       "        1.2378422 ,  0.38447294,  1.1953018 ,  0.7545968 ,  0.5629024 ,\n",
       "        0.44944483,  2.0599785 ,  0.31129   ,  0.43424943], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1932,  0.3865,  0.1932,  0.1932,  0.1932,  0.1932,  0.1932,\n",
       "        0.1932,  2.3192,  0.1932,  0.1932,  0.1932,  0.3865,  0.1932,\n",
       "        0.1932,  0.9663,  0.773 ,  0.773 ,  0.1932,  1.1596,  0.1932,\n",
       "        0.3865,  0.3865,  0.1932,  0.1932,  0.1932,  1.1596,  0.3865,\n",
       "        0.1932,  0.773 ,  0.1932,  0.773 ,  8.6971,  0.1932,  0.3865,\n",
       "        0.5798,  0.1932,  0.1932,  0.1932,  0.1932,  0.9663,  0.1932,\n",
       "        0.1932,  0.1932,  0.1932,  0.3865,  0.1932,  0.9663,  5.2182,\n",
       "        0.3865, 13.5289,  0.1932,  0.1932,  0.1932,  0.1932,  2.3192,\n",
       "        2.7057,  0.9663,  0.1932,  0.3865,  2.7057,  0.1932,  0.1932,\n",
       "        0.1932])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = (tpr -fpr + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5        0.51666667 0.50196078 0.51862745 0.50392157 0.52058824\n",
      " 0.49117647 0.5245098  0.50980392 0.54313725 0.45490196 0.47156863\n",
      " 0.44215686 0.45882353 0.42941176 0.4627451  0.44803922 0.46470588\n",
      " 0.40588235 0.42254902 0.39313725 0.40980392 0.39509804 0.42843137\n",
      " 0.41372549 0.43039216 0.41568627 0.48235294 0.46764706 0.48431373\n",
      " 0.45490196 0.47156863 0.42745098 0.46078431 0.44607843 0.4627451\n",
      " 0.44803922 0.48137255 0.46666667 0.5       ]\n"
     ]
    }
   ],
   "source": [
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "y = np.array([1, 1, 2, 2])\n",
    "scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    @add_start_docstrings_to_callable(BERT_INPUTS_DOCSTRING)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\n",
    "            If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "    Returns:\n",
    "        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`label` is provided):\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n",
    "            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    Examples::\n",
    "        from transformers import BertTokenizer, BertForSequenceClassification\n",
    "        import torch\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
